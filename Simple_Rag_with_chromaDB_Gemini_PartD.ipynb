{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40ab7641b7414fa298774492bc06558e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa1009d82eee4447be7c6dad7fe9696c",
              "IPY_MODEL_94298689dee44cadb1b044d81f77dbce",
              "IPY_MODEL_4b24f9cce141471ea0581908d8cbe44a"
            ],
            "layout": "IPY_MODEL_9f89f4ac9c30466fbef176c41a4764e7"
          }
        },
        "aa1009d82eee4447be7c6dad7fe9696c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac5ddcbce5074f589e6c4fdde96424a7",
            "placeholder": "​",
            "style": "IPY_MODEL_bb9ffcab962f41338ff6030d5503112d",
            "value": "Loading weights: 100%"
          }
        },
        "94298689dee44cadb1b044d81f77dbce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf34f1de50f6459c98907656d7ba485d",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9af99488f424593b6884fbfec52d063",
            "value": 100
          }
        },
        "4b24f9cce141471ea0581908d8cbe44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f11527bd0e94afead74c34de51e181f",
            "placeholder": "​",
            "style": "IPY_MODEL_d916fe86a0774b888cfe9249b29c1f00",
            "value": " 100/100 [00:00&lt;00:00, 488.11it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]"
          }
        },
        "9f89f4ac9c30466fbef176c41a4764e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5ddcbce5074f589e6c4fdde96424a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9ffcab962f41338ff6030d5503112d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf34f1de50f6459c98907656d7ba485d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9af99488f424593b6884fbfec52d063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f11527bd0e94afead74c34de51e181f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d916fe86a0774b888cfe9249b29c1f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b91aff1e8704825a95365a97af886ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2872573d4b7451bb8078d56214cd413",
              "IPY_MODEL_2c72128a6fb747c8b47466bb0d1b5052",
              "IPY_MODEL_6b7159bd7165440eba44c9c950ae1d5c"
            ],
            "layout": "IPY_MODEL_0dfbec637ea14d62ae7123af80731b30"
          }
        },
        "a2872573d4b7451bb8078d56214cd413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1610690e78ec4b9b947e2e78baa96b44",
            "placeholder": "​",
            "style": "IPY_MODEL_97edb208ef9e43d8803ae2e0c314662e",
            "value": "Loading weights: 100%"
          }
        },
        "2c72128a6fb747c8b47466bb0d1b5052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc47d6a16de4fd3862f418543c0aaab",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a78abe788b34acfb8e5060c3f1d5a0c",
            "value": 100
          }
        },
        "6b7159bd7165440eba44c9c950ae1d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_230de8125f714c60a1163365f27eb79d",
            "placeholder": "​",
            "style": "IPY_MODEL_309d045af3d945449a18640574127041",
            "value": " 100/100 [00:00&lt;00:00, 339.87it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]"
          }
        },
        "0dfbec637ea14d62ae7123af80731b30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1610690e78ec4b9b947e2e78baa96b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97edb208ef9e43d8803ae2e0c314662e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbc47d6a16de4fd3862f418543c0aaab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a78abe788b34acfb8e5060c3f1d5a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "230de8125f714c60a1163365f27eb79d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309d045af3d945449a18640574127041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ec0955fa9cf44b5bded6e26a191ed02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2267fdf8f4be42f2b0554d9d5c61de91",
              "IPY_MODEL_045725d3c41f44beaed91bef64f01895",
              "IPY_MODEL_f268cd0893fc4b1e87276a92be540610"
            ],
            "layout": "IPY_MODEL_2d972a4206a54af8b20e86fd0db7be26"
          }
        },
        "2267fdf8f4be42f2b0554d9d5c61de91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69e6cebdb1804db0bdf6d1afe9f81f8f",
            "placeholder": "​",
            "style": "IPY_MODEL_cc7a4afb7b724ab7bce4567612af4d86",
            "value": "Loading weights: 100%"
          }
        },
        "045725d3c41f44beaed91bef64f01895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd22da28c8874326bab26c74db8ed8cd",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d045c2e8cd9b4e87bcf4fe915d2768a6",
            "value": 100
          }
        },
        "f268cd0893fc4b1e87276a92be540610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdd19fdd9078466e9ddaab5b5b7783d7",
            "placeholder": "​",
            "style": "IPY_MODEL_a1a868445046437e8d557cd135fafc7f",
            "value": " 100/100 [00:00&lt;00:00, 246.15it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]"
          }
        },
        "2d972a4206a54af8b20e86fd0db7be26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69e6cebdb1804db0bdf6d1afe9f81f8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7a4afb7b724ab7bce4567612af4d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd22da28c8874326bab26c74db8ed8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d045c2e8cd9b4e87bcf4fe915d2768a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdd19fdd9078466e9ddaab5b5b7783d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1a868445046437e8d557cd135fafc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talha324860/100-pandas-puzzles/blob/master/Simple_Rag_with_chromaDB_Gemini_PartD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART D: A SIMPLE RAG PIPELINE BASED ON GEMINI & CHROMADB\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "ZYN6tFZoaPm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will develop a Retrieval Augmented Generation (RAG) application.\n",
        "\n",
        "The Parts are\n",
        "\n",
        "* PART A: AN INTRO TO GEMINI API FOR TEXT GENERATION & CHAT\n",
        "* PART B: CODE WITH CHROMADB FOR VECTOR STORAGE & SIMILARITY SEARCH\n",
        "* PART C: CODE WITH CHROMADB FOR PERSISTENT VECTOR DB\n",
        "* PART D: A SIMPLE RAG BASED ON GEMINI & CHROMADB\n",
        "* PART E: ADVANCED TECHNIQUES FOR RAG BASED ON GEMINI & CHROMADB"
      ],
      "metadata": {
        "id": "HGmLFS7j8JSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WHAT IS RAG?\n",
        "\n",
        "RAG stands for Retrieval-Augmented Generation. It's a technique that combines large language models (LLMs) with external knowledge sources to improve the accuracy and reliability of AI-generated text.\n",
        "\n",
        "## How Does RAG Work? Unveiling the Power of External Knowledge\n",
        "\n",
        "Before we start the core RAG process, we need to provide a foundation as follows:\n",
        "\n",
        "* **Building the Knowledge Base:** The system starts by transforming documents and information within the external knowledge base (like Wikipedia or a company database) into a special format called **vector representations**. These condense the meaning of each document into a series of **numbers**, capturing the essence of the content.\n",
        "\n",
        "* **Vector Database for Speedy Retrieval**: These vector representations are then stored in a specialized database called a vector database. This database is optimized for efficiently **searching and retrieving** information based on **semantic similarity**. Imagine it as a super-powered library catalog that **understands the meaning** of documents, **not just keywords**.\n",
        "\n",
        "Now, let's explore how RAG leverages this foundation:\n",
        "\n",
        "* **User Input**: The RAG process begins with a question or **prompt** from the user. This could be anything from \"What caused the extinction of the dinosaurs?\" to a more open-ended request like \"Write a creative story.\"\n",
        "\n",
        "* **Intelligent Retrieval**: RAG doesn't rely solely on the **LLM's internal knowledge**. It employs an information retrieval component that acts like a super-powered search engine. This component scans the vast external knowledge base – like a company's internal database for specific domains – to find information **directly relevant** to the user's input. Unlike a traditional **search engine** that relies on **keywords**, RAG leverages the power of vector representations to understand the **semantic meaning** of the user's prompt and identify the most relevant documents.\n",
        "\n",
        "* **Enriched Context Creation**: The retrieved information isn't just shown alongside the prompt. RAG cleverly **merges the user input with the relevant snippets** from the knowledge base. This creates a ***richer context*** for the LLM to understand the **user's intent** and formulate a well-informed response.\n",
        "\n",
        "* **LLM Powered Response Generation**: Finally, the **enriched context** is fed to the Large Language Model (LLM). The LLM, along with its ability to process language patterns, now has a strong **foundation of factual** information to draw upon. This empowers it to generate a response that is both comprehensive and accurate, addressing the specific needs of the user's prompt.\n",
        "\n",
        "In this part, we will learn how to build a persistent ChromaDB Vector Database for speedy retrieval in a Knowledge Base.\n",
        "\n",
        "https://www.trychroma.com/\n",
        "https://github.com/chroma-core/chroma"
      ],
      "metadata": {
        "id": "R91gZB4JIZO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONTENT: A SIMPLE RAG PIPELINE BASED ON GEMINI & CHROMADB\n",
        "\n",
        "In this comprehensive tutorial series, we delve into the exciting world of developing a Retrieval Augmented Generation (RAG) application. If you are eager to create a chatbot leveraging cutting-edge technologies like GEMINI and Chromadb, you are in the right place! This video is tailored for anyone interested in building a RAG system, whether you're a seasoned developer or just starting out.\n",
        "\n",
        "In the first three parts of this series, we explored:\n",
        "\n",
        "* Coding GEMINI API for Text Generation & Chat: Understanding how to implement and use the GEMINI API for creating dynamic text-based interactions.\n",
        "* Creating a Persistent Chromadb for Vector Storage & Similarity Search: Learning how to store and retrieve vectors efficiently using Chromadb.\n",
        "\n",
        "In this fourth installment, titled \"A SIMPLE RAG PIPELINE BASED ON GEMINI & CHROMADB,\" we aim to construct a functional RAG pipeline using these powerful tools. Here's what you can expect:\n",
        "\n",
        "Key Steps Covered in this Video:\n",
        "* Creating a Knowledge Base from Scratch with a Persistent Chromadb: Learn how to build a robust knowledge base from multiple documents.\n",
        "* Upload Multiple Documents and Create Knowledge Base: Step-by-step guide on uploading and organizing your documents.\n",
        "* Test the Knowledge Base: Methods to ensure your knowledge base is functioning correctly.\n",
        "* Load a Knowledge Base from a Persistent Chromadb: How to efficiently load and utilize your knowledge base.\n",
        "* Connect to an LLM: Google GEMINI via the Chat API: Integrate the Google GEMINI model for enhanced interaction.\n",
        "* Create the RAG Pipeline for the Existing Knowledge Base: Develop a seamless pipeline to utilize your knowledge base with GEMINI.\n",
        "* A Simple Loop for User Interaction: Implement a user-friendly loop for interactions.\n",
        "* A Gradio Interface to the RAG: Create an intuitive interface using Gradio for a better user experience.\n",
        "\n",
        "All these steps will be implemented and coded in Python on Google Colab, ensuring you can follow along and replicate the process easily.\n",
        "\n",
        "Follow Us:\n",
        "Murat Karakaya  on LinkedIn\n",
        "Murat Karakaya  on Twitter\n",
        "\n",
        "Join our community of developers and tech enthusiasts! Don't forget to like, share, and subscribe to stay updated with our latest tutorials and tech insights.\n",
        "\n",
        "Watch the video here:\n",
        "* In English:\n",
        "* In Turkish:\n",
        "\n"
      ],
      "metadata": {
        "id": "9Niu38r_Gwqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WHY WE NEED A PERSISTENT CHROMADB?\n",
        "\n",
        "In the context of a Retrieval-Augmented Generation (RAG) approach, saving and loading a persistent ChromaDB is particularly important for several reasons:\n",
        "\n",
        "1. **Enhanced Data Durability**:\n",
        "   - **Importance**: Ensures the retrieval database used for augmenting generative models is not lost between sessions or system restarts.\n",
        "   - **RAG Relevance**: Maintains a consistent and reliable knowledge base that the generative model can reference, leading to more accurate and relevant responses.\n",
        "\n",
        "2. **Operational Continuity**:\n",
        "   - **Importance**: Allows seamless continuation of operations without needing to re-index or re-import data, saving time and computational resources.\n",
        "   - **RAG Relevance**: Ensures that the generative model has continuous access to the same set of documents, which is essential for generating consistent and coherent responses over time.\n",
        "\n",
        "3. **Facilitating Collaboration**:\n",
        "   - **Importance**: Enables multiple users or systems to share and access the same dataset.\n",
        "   - **RAG Relevance**: Supports collaborative development and usage of the RAG system, allowing different teams to work on improving the retrieval and generation processes simultaneously.\n",
        "\n",
        "4. **Scalability**:\n",
        "   - **Importance**: Provides a stable and persistent backend, enabling efficient handling of large datasets.\n",
        "   - **RAG Relevance**: Essential for scaling the RAG system to handle more extensive and diverse knowledge bases, ensuring that the system can manage increased loads and deliver prompt, relevant information.\n",
        "\n",
        "\n",
        "In a RAG system, the retriever (like ChromaDB) provides the generative model with relevant context from a knowledge base to generate informed and accurate responses. Persistent storage ensures that this knowledge base is durable, continuously available, and scalable, which is critical for the reliability, consistency, and performance of the RAG system.\n",
        "\n"
      ],
      "metadata": {
        "id": "d0jFbbAH8qa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING A KNOWLEDGE BASE FROM SCRATCH WITH A PERSISTENT CHROMADB\n",
        "\n",
        "To make ChromaDB durable (persistent) rather than temporary on Google Colab, you can use external storage services like Google Drive or set up a cloud-based database. Google Colab provides temporary storage that resets after each session, so to maintain persistence across sessions, you'll need to save your data and configurations externally."
      ],
      "metadata": {
        "id": "____WNA_3LAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 Install required libraries\n",
        "\n",
        "Install all the required libraries and helper functions"
      ],
      "metadata": {
        "id": "TU6DsMABRbJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install chromadb --quiet\n",
        "%pip install sentence_transformers --quiet\n",
        "%pip install pypdf --quiet\n",
        "%pip install langchain --quiet\n",
        "%pip install tqdm --quiet\n",
        "%pip install -U langchain langchain-community langchain-text-splitters\n",
        "\n",
        "\n",
        "import langchain\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
        "\n",
        "\n",
        "from pypdf import PdfReader\n",
        "\n",
        "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
        "from chromadb import Client, PersistentClient\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "loqDKFVeRSKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772ea5e5-172d-488e-e13d-272a3be2c056"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.11)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.9)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Initialize a Persistent ChromaDB client with a proper Google Drive connection"
      ],
      "metadata": {
        "id": "dZ595WNn3ccX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5Krf5TF8pbg",
        "outputId": "3b5ef96c-ae34-471a-f472-a4e1e1120a52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/'Colab Notebooks'"
      ],
      "metadata": {
        "id": "jRLnsJpB8y_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d049f4-a1b0-45b4-88ea-67d64d87eeb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ChromaDB client with Google Drive connection\n",
        "chromaDB_path = '/content/drive/MyDrive/Colab Notebooks/ChromaDBData'\n"
      ],
      "metadata": {
        "id": "bohCxC0D9p_P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the chromadb_path exists or not. If so, delete all the files and folders in chromadb_path. But before deleting get the permission from the user.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def delete_all_files_and_folders(chromaDB_path):\n",
        "  if os.path.exists(chromaDB_path):\n",
        "    print(f\"The directory '{chromaDB_path}' already exists.\")\n",
        "    permission = input(\"Do you want to delete all the files and folders in this directory? (y/n): \")\n",
        "    if permission == \"y\":\n",
        "      shutil.rmtree(chromaDB_path)\n",
        "      print(f\"All files and folders in '{chromaDB_path}' have been deleted.\")\n",
        "    else:\n",
        "      print(\"No action taken.\")\n",
        "  else:\n",
        "    print(f\"The directory '{chromaDB_path}' does not exist.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rX7s6ZG2clxy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_all_files_and_folders(chromaDB_path)"
      ],
      "metadata": {
        "id": "SJLnNnXET71j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40396e53-b245-4884-9788-e658e1f4dfd3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory '/content/drive/MyDrive/Colab Notebooks/ChromaDBData' already exists.\n",
            "Do you want to delete all the files and folders in this directory? (y/n): y\n",
            "All files and folders in '/content/drive/MyDrive/Colab Notebooks/ChromaDBData' have been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Define PersistentClient\n",
        "\n",
        "Let's re-define the **create_chroma_client** function from the previous part so that this time we initialize a **persistent** ChromaDB client:"
      ],
      "metadata": {
        "id": "_Z7mIpAt-bCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
        "from chromadb import Client, PersistentClient\n"
      ],
      "metadata": {
        "id": "mo8-L45AAxBy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chroma_client(collection_name, embedding_function, chromaDB_path ):\n",
        "  if chromaDB_path is not None:\n",
        "    chroma_client = PersistentClient(path=chromaDB_path,\n",
        "                                     settings=Settings(),\n",
        "                                     tenant=DEFAULT_TENANT,\n",
        "                                     database=DEFAULT_DATABASE,)\n",
        "  else:\n",
        "    chroma_client = Client()\n",
        "\n",
        "  chroma_collection = chroma_client.get_or_create_collection(\n",
        "      collection_name,\n",
        "      embedding_function=embedding_function)\n",
        "\n",
        "  return chroma_client, chroma_collection"
      ],
      "metadata": {
        "id": "IFDiy4aw-Z1n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Create a collection as usual"
      ],
      "metadata": {
        "id": "i4z_PaKy34WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"Papers\"\n",
        "sentence_transformer_model=\"distiluse-base-multilingual-cased-v1\"\n",
        "embedding_function= embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=sentence_transformer_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "40ab7641b7414fa298774492bc06558e",
            "aa1009d82eee4447be7c6dad7fe9696c",
            "94298689dee44cadb1b044d81f77dbce",
            "4b24f9cce141471ea0581908d8cbe44a",
            "9f89f4ac9c30466fbef176c41a4764e7",
            "ac5ddcbce5074f589e6c4fdde96424a7",
            "bb9ffcab962f41338ff6030d5503112d",
            "cf34f1de50f6459c98907656d7ba485d",
            "d9af99488f424593b6884fbfec52d063",
            "3f11527bd0e94afead74c34de51e181f",
            "d916fe86a0774b888cfe9249b29c1f00"
          ]
        },
        "id": "TOrsU6-nSOD_",
        "outputId": "4cca7a23-2317-4fcd-a6b5-ec76383c1249"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40ab7641b7414fa298774492bc06558e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client, chroma_collection = create_chroma_client(collection_name,\n",
        "                                                        embedding_function,\n",
        "                                                        chromaDB_path)"
      ],
      "metadata": {
        "id": "pN-vX46885Tb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the created collection:"
      ],
      "metadata": {
        "id": "DU8ot87UMYMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify collection properties\n",
        "print(f\"Collection name: {chroma_collection.name}\")\n",
        "print(f\"Number of documents in collection: {chroma_collection.count()}\")\n",
        "\n",
        "# List all collections in the client\n",
        "print(\"All collections in ChromaDB client:\")\n",
        "for collection in chroma_client.list_collections():\n",
        "    print(collection.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1kr0pgfMQjT",
        "outputId": "7f8855ae-e25a-4439-d113-e05834804386"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection name: Papers\n",
            "Number of documents in collection: 0\n",
            "All collections in ChromaDB client:\n",
            "Papers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Define helper functions"
      ],
      "metadata": {
        "id": "hQFAEwB2TYO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "def upload_multiple_files():\n",
        "  uploaded = files.upload()\n",
        "  file_names = list()\n",
        "  for fn in uploaded.keys():\n",
        "    #print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "    file_names.append(fn)\n",
        "  return file_names"
      ],
      "metadata": {
        "id": "miZpcDP7S5yM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_PDF_Text(pdf_path):\n",
        "  reader = PdfReader(pdf_path)\n",
        "  pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
        "  # Filter the empty strings\n",
        "  pdf_texts = [text for text in pdf_texts if text]\n",
        "  print(\"Document: \",pdf_path,\" chunk size: \", len(pdf_texts))\n",
        "  return pdf_texts"
      ],
      "metadata": {
        "id": "6Qf8RKItTSp9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_Page_ChunkinChar(pdf_texts, chunk_size = 1500, chunk_overlap=0 ):\n",
        "  character_splitter = RecursiveCharacterTextSplitter(\n",
        "      separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "      chunk_size=1500,\n",
        "      chunk_overlap=0\n",
        ")\n",
        "  character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
        "  print(f\"\\nTotal number of chunks (document splited by max char = 1500): \\\n",
        "        {len(character_split_texts)}\")\n",
        "  return character_split_texts"
      ],
      "metadata": {
        "id": "irVYmZf9T4Lc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_Chunk_Token(text_chunksinChar,sentence_transformer_model, chunk_overlap=0,tokens_per_chunk=128 ):\n",
        "  token_splitter = SentenceTransformersTokenTextSplitter(\n",
        "      chunk_overlap=0,\n",
        "      model_name=sentence_transformer_model,\n",
        "      tokens_per_chunk=128)\n",
        "\n",
        "  text_chunksinTokens = []\n",
        "  for text in text_chunksinChar:\n",
        "      text_chunksinTokens += token_splitter.split_text(text)\n",
        "  print(f\"\\nTotal number of chunks (document splited by 128 tokens per chunk):\\\n",
        "       {len(text_chunksinTokens)}\")\n",
        "  return text_chunksinTokens"
      ],
      "metadata": {
        "id": "ejyPbZG6ULBR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_meta_data(text_chunksinTokens, title, category, initial_id):\n",
        "  ids = [str(i+initial_id) for i in range(len(text_chunksinTokens))]\n",
        "  metadata = {\n",
        "      'document': title,\n",
        "      'category': category\n",
        "  }\n",
        "  metadatas = [ metadata for i in range(len(text_chunksinTokens))]\n",
        "  return ids, metadatas"
      ],
      "metadata": {
        "id": "Ag7EWuVRU10F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_document_to_collection(ids, metadatas, text_chunksinTokens, chroma_collection):\n",
        "  print(\"Before inserting, the size of the collection: \", chroma_collection.count())\n",
        "  chroma_collection.add(ids=ids, metadatas= metadatas, documents=text_chunksinTokens)\n",
        "  print(\"After inserting, the size of the collection: \", chroma_collection.count())\n",
        "  return chroma_collection"
      ],
      "metadata": {
        "id": "txJWVJPBVMMM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieveDocs(chroma_collection, query, n_results=5, return_only_docs=False):\n",
        "    results = chroma_collection.query(query_texts=[query],\n",
        "                                      include= [ \"documents\",\"metadatas\",'distances' ],\n",
        "                                      n_results=n_results)\n",
        "\n",
        "    if return_only_docs:\n",
        "        return results['documents'][0]\n",
        "    else:\n",
        "        return results"
      ],
      "metadata": {
        "id": "SGVfxSvkZEvD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(results, return_only_docs=False):\n",
        "\n",
        "  if return_only_docs:\n",
        "    retrieved_documents = results\n",
        "    if len(retrieved_documents) == 0:\n",
        "      print(\"No results found.\")\n",
        "      return\n",
        "    for i, doc in enumerate(retrieved_documents):\n",
        "      print(f\"Document {i+1}:\")\n",
        "      print(\"\\tDocument Text: \")\n",
        "      display(to_markdown(doc));\n",
        "  else:\n",
        "\n",
        "      retrieved_documents = results['documents'][0]\n",
        "      if len(retrieved_documents) == 0:\n",
        "          print(\"No results found.\")\n",
        "          return\n",
        "      retrieved_documents_metadata = results['metadatas'][0]\n",
        "      retrieved_documents_distances = results['distances'][0]\n",
        "      print(\"------- retreived documents -------\\n\")\n",
        "\n",
        "      for i, doc in enumerate(retrieved_documents):\n",
        "          print(f\"Document {i+1}:\")\n",
        "          print(\"\\tDocument Text: \")\n",
        "          display(to_markdown(doc));\n",
        "          print(f\"\\tDocument Source: {retrieved_documents_metadata[i]['document']}\")\n",
        "          print(f\"\\tDocument Source Type: {retrieved_documents_metadata[i]['category']}\")\n",
        "          print(f\"\\tDocument Distance: {retrieved_documents_distances[i]}\")\n"
      ],
      "metadata": {
        "id": "E2IwDowkZLvK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_multiple_pdfs_to_ChromaDB(collection_name,sentence_transformer_model,\n",
        "                                   chromaDB_path):\n",
        "\n",
        "  collection_name= collection_name\n",
        "  category= \"Journal Paper\"\n",
        "  sentence_transformer_model=sentence_transformer_model\n",
        "  embedding_function= embedding_functions.SentenceTransformerEmbeddingFunction(model_name=sentence_transformer_model)\n",
        "  chroma_client, chroma_collection = create_chroma_client(collection_name, embedding_function, chromaDB_path)\n",
        "  current_id = chroma_collection.count()\n",
        "  file_names = upload_multiple_files()\n",
        "  for file_name in file_names:\n",
        "    print(f\"Document: {file_name} is being processed to be added to the {chroma_collection.name} {chroma_collection.count()}\")\n",
        "    print(f\"current_id: {current_id} \")\n",
        "    pdf_texts = convert_PDF_Text(file_name)\n",
        "    text_chunksinChar = convert_Page_ChunkinChar(pdf_texts)\n",
        "    text_chunksinTokens = convert_Chunk_Token(text_chunksinChar,sentence_transformer_model)\n",
        "    ids,metadatas = add_meta_data(text_chunksinTokens,file_name,category, current_id)\n",
        "    current_id = current_id + len(text_chunksinTokens)\n",
        "    chroma_collection = add_document_to_collection(ids, metadatas, text_chunksinTokens, chroma_collection)\n",
        "    print(f\"Document: {file_name} added to the collection: {chroma_collection.count()}\")\n",
        "  return  chroma_client, chroma_collection"
      ],
      "metadata": {
        "id": "U_Dy4QIcCSqi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Upload Multiple Documents and Create Knowledge Base\n",
        "\n",
        "Run load_multiple_pdfs_to_ChromaDB() to fill in the colection"
      ],
      "metadata": {
        "id": "_P7eumIj4VoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client, chroma_collection= load_multiple_pdfs_to_ChromaDB(collection_name,sentence_transformer_model, chromaDB_path)"
      ],
      "metadata": {
        "id": "nJL5leeq8yd_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574,
          "referenced_widgets": [
            "3b91aff1e8704825a95365a97af886ce",
            "a2872573d4b7451bb8078d56214cd413",
            "2c72128a6fb747c8b47466bb0d1b5052",
            "6b7159bd7165440eba44c9c950ae1d5c",
            "0dfbec637ea14d62ae7123af80731b30",
            "1610690e78ec4b9b947e2e78baa96b44",
            "97edb208ef9e43d8803ae2e0c314662e",
            "cbc47d6a16de4fd3862f418543c0aaab",
            "3a78abe788b34acfb8e5060c3f1d5a0c",
            "230de8125f714c60a1163365f27eb79d",
            "309d045af3d945449a18640574127041",
            "6ec0955fa9cf44b5bded6e26a191ed02",
            "2267fdf8f4be42f2b0554d9d5c61de91",
            "045725d3c41f44beaed91bef64f01895",
            "f268cd0893fc4b1e87276a92be540610",
            "2d972a4206a54af8b20e86fd0db7be26",
            "69e6cebdb1804db0bdf6d1afe9f81f8f",
            "cc7a4afb7b724ab7bce4567612af4d86",
            "dd22da28c8874326bab26c74db8ed8cd",
            "d045c2e8cd9b4e87bcf4fe915d2768a6",
            "fdd19fdd9078466e9ddaab5b5b7783d7",
            "a1a868445046437e8d557cd135fafc7f"
          ]
        },
        "outputId": "67abfa86-a3b1-4832-db76-92b1c492c4e9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9fed87a8-1079-4c98-b4c0-1604e4261f6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9fed87a8-1079-4c98-b4c0-1604e4261f6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1912.10819v1.pdf to 1912.10819v1 (2).pdf\n",
            "Saving peerj-cs-93.pdf to peerj-cs-93 (2).pdf\n",
            "Document: 1912.10819v1 (2).pdf is being processed to be added to the Papers 0\n",
            "current_id: 0 \n",
            "Document:  1912.10819v1 (2).pdf  chunk size:  12\n",
            "\n",
            "Total number of chunks (document splited by max char = 1500):         27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b91aff1e8704825a95365a97af886ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of chunks (document splited by 128 tokens per chunk):       73\n",
            "Before inserting, the size of the collection:  0\n",
            "After inserting, the size of the collection:  73\n",
            "Document: 1912.10819v1 (2).pdf added to the collection: 73\n",
            "Document: peerj-cs-93 (2).pdf is being processed to be added to the Papers 73\n",
            "current_id: 73 \n",
            "Document:  peerj-cs-93 (2).pdf  chunk size:  19\n",
            "\n",
            "Total number of chunks (document splited by max char = 1500):         48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ec0955fa9cf44b5bded6e26a191ed02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of chunks (document splited by 128 tokens per chunk):       126\n",
            "Before inserting, the size of the collection:  73\n",
            "After inserting, the size of the collection:  199\n",
            "Document: peerj-cs-93 (2).pdf added to the collection: 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 Test the Knowledge Base\n",
        "\n",
        "Query the Knowledge Base using the persistent ChromaDB client and & collection"
      ],
      "metadata": {
        "id": "SY_pADjm4dle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the methods of predict desicion of echr cases?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ZB7_29IHZXtK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_documents=retrieveDocs(chroma_collection, query, 10)\n",
        "show_results(retrieved_documents)"
      ],
      "metadata": {
        "id": "fbpduFllB8I6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be835ad0-d01a-4fdb-e544-5872530731a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- retreived documents -------\n",
            "\n",
            "Document 1:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> on Test Set The models could still be used to prioritise cases by identifying which cases are more likely to lead to violations. The heuristic does not provide any bene [UNK] in terms of prioritising cases. As the predictions for each Article would be the same, all complaints would be given the same priority. In this sense, the models may be more useful. As discussed above, the tendency to have a high precision means there are relatively few false positives. This means the cases identi [UNK] as violations and subsequently prioritised, will tend to be violations. The downside is"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: 1912.10819v1 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.6878165006637573\n",
            "Document 2:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> behavior conforms to the legal realists [UNK] theorization ( Leiter, 2007 ), according to which judges primarily decide cases by responding to the stimulus of the facts of the case. We define the problem of the ECtHR case prediction as a binary classification task. We utilise textual features, i. e., N - grams and topics, to train Support Vector Machine ( SVM ) classifiers ( Vapnik, 1998 ). We apply a linear kernel function that facilitates the interpretation of models in a straightforward manner. Our models can reliably predict ECtH"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.6883445978164673\n",
            "Document 3:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> 5Rules of ECtHR, http : / / www. echr. coe. int / Documents / Rules _ Court _ ENG. pdf. Main premise Our main premise is that published judgments can be used to test the possibility of a text - based analysis for ex ante predictions of outcomes on the assumption that there is enough similarity between ( at least ) certain chunks of the text of published judgments and applications lodged with the Court and / or briefs submitted by parties with respect to pending cases. Predictive tasks were based on the text of published judgments rather"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.692875862121582\n",
            "Document 4:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> cases. We submit, though, that full acceptance of that reasonable assumption necessitates more empirical corroboration. Be that as it may, our more general aim is to work under this assumption, thus placing our work within the larger context of ongoing empirical research in the theory of adjudication about the determinants of judicial decision - making. Accordingly, in the discussion we highlight ways in which automatically predicting the outcomes of ECtHR cases could potentially provide insights on whether judges follow a so - called legal model ( Grey, 1983 ) of decision making or their"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.6982980966567993\n",
            "Document 5:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> and predict the outcomes of judicial decisions ( Lawlor, 1963 ). According to Lawlor, reliable prediction of the activity of judges would depend on a scientific understanding of the ways"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.7002371549606323\n",
            "Document 6:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Preoţiuc - Pietro, Lampos & Aletras, 2015 ; Preoţiuc - Pietro et al., 2015 ) and also provides a more concise semantic representation. [UNK] model The problem of predicting the decisions of the ECtHR is defined as a binary classification task. Our goal is to predict if, in the context of a particular case, there is a violation or non - violation in relation to a specific Article of the Convention. For that purpose, we use each set of textual features, i. e., N - grams and topics, to train Support Ve"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.7054527997970581\n",
            "Document 7:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> particularity. Boston University Law Review 78 : 773. Segal JA. 1984. Predicting Supreme Court cases probabilistically : the search and seizure cases, 1962 [UNK] 1981. American Political Science Review 78 ( 04 ) : 891 [UNK] 900 DOI 10. 2307 / 1955796. Segal JA, Spaeth HJ. 2002. The Supreme Court and the attitudinal model revisited. Cambridge : Cambridge University Press. Aletras etal ( 2016 ), PeerJ Comput. Sci., DOI 10. 7717 / peerj - cs. 93 18 / 19"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.7078721523284912\n",
            "Document 8:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Such a model could also be used to prioritise cases. That is cases which indicate a high likelihood of violation can be prioritised. 2 Related Work Table 1 shows the results of studies that looked at predicting the outcome of legal cases. The [UNK] Court [UNK] column gives the legal court considered by the study. The majority of the studies looked at either the ECHR or the Supreme Court of the United States ( SCOTUS ). The [UNK] Data [UNK] column in Table 1 givesthe type of data used in the study. [UNK] Case documents [UNK] refers to text documents that outline the cases heard by the Court"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: 1912.10819v1 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.7101020812988281\n",
            "Document 9:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ##ly robust manner patterns of fact scenarios that correspond to well - established trends in the Court [UNK] s case law. Aletras etal ( 2016 ), PeerJ Comput. Sci., DOI 10. 7717 / peerj - cs. 93 12 / 19"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.7140823602676392\n",
            "Document 10:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> number of cases. In the Court [UNK] s own words :"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: 1912.10819v1 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.7207286357879639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 Observe the ChromeDB saved to the provided path\n",
        "\n",
        "List the folders and files in the chromaDB_path"
      ],
      "metadata": {
        "id": "Vn6S6rAS4lhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"{chromaDB_path}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJfd2hl85T4N",
        "outputId": "5821eebd-7dea-4ae7-e990-310c457672e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71f158d0-357e-4e73-8695-f67c010fc8e4  chroma.sqlite3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YES WE DID IT!"
      ],
      "metadata": {
        "id": "UUNm9qIi4sWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD A KNOWLEDGE BASE FROM A PERSISTENT CHROMADB\n",
        "\n"
      ],
      "metadata": {
        "id": "lCZkStQJsWU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's kill the kernel so we ensure that nothing remains in the memory from all the above ChromaDB instance."
      ],
      "metadata": {
        "id": "v-gnovuy65Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "# Disconnect from the runtime\n",
        "#!kill -9 -1"
      ],
      "metadata": {
        "id": "XuO4UOZ76JAq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 Connect to source directory\n",
        "\n",
        "First get connected to the ChromaDB directory"
      ],
      "metadata": {
        "id": "eQQYkyA_0hi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0M0aAIB9tK8q",
        "outputId": "75776596-9631-4b0a-86e8-7e756873c6de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change directory to chromaDB folder\n",
        "chromaDB_path = '/content/drive/MyDrive/Colab Notebooks/ChromaDBData'\n",
        "%cd {chromaDB_path}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW_FIIWRuPFF",
        "outputId": "3493c460-9a90-4d52-8017-a56bc807c20d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/ChromaDBData\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check that if chromadb_path exist or not and if exists does it contain chromadb files and folders"
      ],
      "metadata": {
        "id": "Dg1hEUMQw0wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists(chromaDB_path):\n",
        "    print(f\"The directory '{chromaDB_path}' exists.\")\n",
        "\n",
        "    # Check if the directory contains ChromaDB files and folders\n",
        "    chromadb_files_and_folders = os.listdir(chromaDB_path)\n",
        "    if any(file_or_folder.startswith('chroma') for file_or_folder in chromadb_files_and_folders):\n",
        "        print(\"The directory contains ChromaDB files and folders.\")\n",
        "    else:\n",
        "        print(\"The directory does not contain ChromaDB files and folders.\")\n",
        "else:\n",
        "    print(f\"The directory '{chromadb_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaMkHKPLwW_J",
        "outputId": "663f144d-e408-4cb6-b312-38219581473b",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory '/content/drive/MyDrive/Colab Notebooks/ChromaDBData' exists.\n",
            "The directory contains ChromaDB files and folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2 Install required libraries\n",
        "\n",
        "Secondly install all the required libraries and helper functions"
      ],
      "metadata": {
        "id": "jxr8k00A0uyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install chromadb --quiet\n",
        "%pip install sentence_transformers --quiet"
      ],
      "metadata": {
        "id": "8Q-aNfaT0Pz0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
        "from chromadb import Client, PersistentClient\n",
        "from chromadb.utils import embedding_functions\n"
      ],
      "metadata": {
        "id": "zCT7F72PseyP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "tPsQ2-K20CmE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieveDocs(chroma_collection, query, n_results=5,\n",
        "                 return_only_docs=False, filterType=None, filterValue=None):\n",
        "    if filterType is not None and filterValue is not None:\n",
        "        results = chroma_collection.query(\n",
        "            query_texts=[query],\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"],\n",
        "            where={filterType: filterValue},\n",
        "            n_results=n_results)\n",
        "\n",
        "    else:\n",
        "        results = chroma_collection.query(\n",
        "            query_texts=[query],\n",
        "            include= [ \"documents\",\"metadatas\",'distances' ],\n",
        "            n_results=n_results)\n",
        "\n",
        "    if return_only_docs:\n",
        "        return results['documents'][0]\n",
        "    else:\n",
        "        return results"
      ],
      "metadata": {
        "id": "xh4T7meazUSF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(results, return_only_docs=False):\n",
        "\n",
        "  if return_only_docs:\n",
        "    retrieved_documents = results\n",
        "    if len(retrieved_documents) == 0:\n",
        "      print(\"No results found.\")\n",
        "      return\n",
        "    for i, doc in enumerate(retrieved_documents):\n",
        "      print(f\"Document {i+1}:\")\n",
        "      print(\"\\tDocument Text: \")\n",
        "      display(to_markdown(doc));\n",
        "  else:\n",
        "\n",
        "      retrieved_documents = results['documents'][0]\n",
        "      if len(retrieved_documents) == 0:\n",
        "          print(\"No results found.\")\n",
        "          return\n",
        "      retrieved_documents_metadata = results['metadatas'][0]\n",
        "      retrieved_documents_distances = results['distances'][0]\n",
        "      print(\"------- retreived documents -------\\n\")\n",
        "\n",
        "      for i, doc in enumerate(retrieved_documents):\n",
        "          print(f\"Document {i+1}:\")\n",
        "          print(\"\\tDocument Text: \")\n",
        "          display(to_markdown(doc));\n",
        "          print(f\"\\tDocument Source: {retrieved_documents_metadata[i]['document']}\")\n",
        "          print(f\"\\tDocument Source Type: {retrieved_documents_metadata[i]['category']}\")\n",
        "          print(f\"\\tDocument Distance: {retrieved_documents_distances[i]}\")\n"
      ],
      "metadata": {
        "id": "4FPBhuz_zlFE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 Initailizing\n",
        "\n",
        " Now, we can begin to upload the persistent ChromaDB from the location by initailizing\n",
        "*  the chromaDB client\n",
        "*  the chromaDB collections"
      ],
      "metadata": {
        "id": "Mn3OPY0r1axg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = PersistentClient(path=chromaDB_path,\n",
        "                                     settings=Settings(),\n",
        "                                     tenant=DEFAULT_TENANT,\n",
        "                                     database=DEFAULT_DATABASE,)"
      ],
      "metadata": {
        "id": "iL-3d53fB7c6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client.list_collections()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX3GuXTpB69J",
        "outputId": "b10293df-b648-4ec7-987d-ff040d575620"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Collection(name=Papers)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"Papers\"\n",
        "sentence_transformer_model=\"distiluse-base-multilingual-cased-v1\"\n",
        "embedding_function= embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=sentence_transformer_model)\n"
      ],
      "metadata": {
        "id": "ZsgtHESYvq80"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_collection = chroma_client.get_or_create_collection(\n",
        "      collection_name,\n",
        "      embedding_function=embedding_function)"
      ],
      "metadata": {
        "id": "bSAESLFfrosn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify collection properties\n",
        "print(f\"Collection name: {chroma_collection.name}\")  # Access the name attribute directly\n",
        "print(f\"Number of documents in collection: {chroma_collection.count()}\")\n",
        "\n",
        "# List all collections in the client\n",
        "print(\"All collections in ChromaDB client:\")\n",
        "for collection in chroma_client.list_collections():\n",
        "    print(collection.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkAXDiZ_OvDz",
        "outputId": "f995e279-e702-48ab-da15-39ae8dea22a6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection name: Papers\n",
            "Number of documents in collection: 199\n",
            "All collections in ChromaDB client:\n",
            "Papers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4 Test\n",
        "\n",
        "Test the loaded ChromeDB client and the collection"
      ],
      "metadata": {
        "id": "VPYaFV8a12YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_collection.get(['0'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPHuH6bbLbED",
        "outputId": "d67ff586-9c2a-4089-ba0b-1b410bb88f20"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['0'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['Conor O [UNK] Sullivan and Joeran Beel. [UNK] Predicting the Outcome of Judicial Decisions made by the European Court of Human Rights. [UNK] In 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science, 2019. Predicting the Outcome of Judicial Decisions made by the European Court of Human Rights Conor O [UNK] Sullivan, Joeran Beel School of Computer Science and Statistics, Trinity College, Ireland ADAPT Centre osullc43©tcd. ie, beelj©tcd. ie Abstract. In this study, machine learning models were constructed'],\n",
              " 'uris': None,\n",
              " 'included': ['metadatas', 'documents'],\n",
              " 'data': None,\n",
              " 'metadatas': [{'category': 'Journal Paper',\n",
              "   'document': '1912.10819v1 (2).pdf'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "chroma_collection.get(['0'])\n",
        "\n",
        "{'ids': ['0'],\n",
        " 'embeddings': None,\n",
        " 'metadatas': [{'category': 'Journal Paper',\n",
        "   'document': '15 UAV Route Planning For Maximum Target Coverage.pdf'}],\n",
        " 'documents': ['Computer Science & Engineering : An International Journal ( CSEIJ ), Vol. 4, No. 1,\n",
        " February 2014 DOI : 10. 5121 / cseij. 2014. 410 3 27UAVROUTEPLANNING FORMAXIMUMTARGET COVERAGE MuratKarakaya1\n",
        " 1Department of Computer Engineering, Atilim University, Ankara, Turkey ABSTRACT Utilization of Unmanned Aerial\n",
        " Vehicles ( UAVs ) in military and civil operations is getting popular. One of the challenges in effectively\n",
        "  tasking these expensive vehicles is planning'],\n",
        " 'uris': None,\n",
        " 'data': None}\n",
        "```"
      ],
      "metadata": {
        "id": "UL3rKoy9x8n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Target Coverage?\""
      ],
      "metadata": {
        "id": "U2NsqZQwy9mx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_documents=retrieveDocs(chroma_collection, query, 10)\n",
        "show_results(retrieved_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bo0CKmb2KJ_A",
        "outputId": "df952f99-ccea-441b-8d29-f9ba2f854bda"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- retreived documents -------\n",
            "\n",
            "Document 1:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> . The majority of the studies that looked at SCOTUS decisionsused [UNK] Summary Information [UNK] [ 22 ] [ 10 ] [ 11 ] [ 12 ]. Thesearevariablesthat summarise the cases. Additionally, in Table 1 the [UNK] Target Variable [UNK] is usually a simpli [UNK] of potential case outcomes. For each study, the [UNK] Algorithm [UNK] gives the algorithm that achieved the highest accuracy when predicting the target variable. The [UNK] Train. Ace. [UNK] and [UNK] Test Acc. [UNK] give the training and test accuracy, respectfully, achieved by the study. CForall the studies, the"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: 1912.10819v1 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.8299545049667358\n",
            "Document 2:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i. e., N - grams, and topics. Our models can predict the court [UNK] s decisions with a strong accuracy ( 79 % on average ). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision -"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.857610821723938\n",
            "Document 3:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Table 1. Summary of Previous Works Target. Train. Test Author Court Data Variable Algorlthm Acc. Acc. [ 3 ] ECHR Case [UNK] damn [UNK] SVM 801 [UNK] 7 NADocuments Non [UNK] Violation ' 0 Case Violation, [ 15 ] ECHR Documents Non [UNK] Violation SVM 795 % NA Case Violation, [ 18 ] ECHR Documents Non [UNK] Violation SVM 750 % 740 % Summary Af [UNK], Decision [ 17 ] SCOTUS Information Reversed Tree NA 75 % Justice. Sumamry Decsion : Stocha"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: 1912.10819v1 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.8728786706924438\n",
            "Document 4:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Preoţiuc - Pietro, Lampos & Aletras, 2015 ; Preoţiuc - Pietro et al., 2015 ) and also provides a more concise semantic representation. [UNK] model The problem of predicting the decisions of the ECtHR is defined as a binary classification task. Our goal is to predict if, in the context of a particular case, there is a violation or non - violation in relation to a specific Article of the Convention. For that purpose, we use each set of textual features, i. e., N - grams and topics, to train Support Ve"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.8749464750289917\n",
            "Document 5:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> way, the representation of the Full case is computed by taking the mean vector of all of its sub - parts.   * Topics : We create topics for each article by clustering together N - grams that are semantically similar by leveraging the distributional hypothesis suggesting that similar words appear in similar contexts. We thus use the C feature matrix ( see above ), which is a distributional representation ( Turney & Pantel, 2010 ) of the N - grams given the case as the context ; each column vector of the matrix represents an N - gram. Using this vector representation of words, we comput"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.8913159966468811\n",
            "Document 6:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> than lodged applications or briefs simply because we did not have access to the relevant data set. We thus used published judgments as proxies for the material to which we do not have access. This point should be borne in mind when approaching our results. At the very least, our work can be read in the following hypothetical way : if there is enough similarity between the chunks of text of published judgments that we analyzed and that of lodged applications and briefs, then our approach can be fruitfully used to predict outcomes with these other kinds of texts. Case structure The judgments"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.9108981490135193\n",
            "Document 7:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> a single topic. A representation of a cluster is derived by looking at the most frequent N - grams it contains. The main advantages of using topics ( sets of N - grams ) instead of single N - grams is that it reduces the dimensionality of the feature space, which is essential for feature selection, it limits overfitting to training data ( Lampos et al., 2014 ;"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.9155511856079102\n",
            "Document 8:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> crucial clarifications and caveats should be stressed. To begin with, the text of the [UNK] Circumstances [UNK] subsection has been formulated by the Court itself. As a result, it should not always be understood as a neutral mirroring of the factual background of the case. The choices made by the Court when it comes to formulations of the facts incorporate implicit or explicit judgments to the effect that some facts are more Aletras etal ( 2016 ), PeerJ Comput. Sci., DOI 10. 7717 / peerj - cs. 93 4 / 19"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.92230224609375\n",
            "Document 9:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ##section, with all the caveats we have already voiced, is a ( crude ) proxy for non - legal facts and the [UNK] Law [UNK] subsection is a ( crude ) proxy for legal reasons and arguments, the predictive superiority of the [UNK] Circumstances [UNK] subsection seems to cohere with extant legal realist treatments of judicial decision - making."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: peerj-cs-93 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.9234130382537842\n",
            "Document 10:\n",
            "\tDocument Text: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> dings and [UNK] No. Tokens [UNK] are the number of lower case words that make up the corpus. The [UNK] vocabulary size [UNK] is the number of words that have vector repre [UNK] sentations for that embedding. For each embedding, a 100 dimension and 200 dimension version are used. The GloVe embeddings were trained by [ 20 ] and the law2vec embeddings were trained by [ 4 ]. The echr2vec embeddings were trained speci [UNK] for this paper."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDocument Source: 1912.10819v1 (2).pdf\n",
            "\tDocument Source Type: Journal Paper\n",
            "\tDocument Distance: 0.9250677227973938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YES! WE DID IT!"
      ],
      "metadata": {
        "id": "MqZPe52R2AZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONNECT TO AN LLM: GOOGLE GEMINI"
      ],
      "metadata": {
        "id": "MmwlLvI3QHum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Install & Import Libraries"
      ],
      "metadata": {
        "id": "lA9pxZ5CQTDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-genai"
      ],
      "metadata": {
        "id": "Jeo267kygb5T",
        "outputId": "e2afae04-d11d-4ae3-c367-94aafebb56fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.63.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "WkciE5EKnoLg",
        "outputId": "72b726bf-39f0-4a02-e443-c1faf9b457f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "bu5XjXDAQm2U",
        "outputId": "3f100c49-4f20-4d82-df83-1adbf6b1f466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Define Helper Functions"
      ],
      "metadata": {
        "id": "2qDkZNw_QtLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "oD9QAXBMQw41"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_chatBot(system_instruction):\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name='gemini-2.5-flash',\n",
        "        system_instruction=system_instruction\n",
        "    )\n",
        "    chat = model.start_chat(history=[])\n",
        "    return chat"
      ],
      "metadata": {
        "id": "Wfm2jvLTRED-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_LLM_answer(prompt, context, chat):\n",
        "  response = chat.send_message( prompt + context)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "6onefHglRakd"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Connect to the LLM via the Chat API"
      ],
      "metadata": {
        "id": "GNvdN-SNQ1em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('geminiapikey')\n",
        "client = genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "1JgPySYbQ_fk"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\" You are an attentive and supportive academic assistant.\n",
        "Your role is to provide assistance based solely on the provided context.\n",
        "\n",
        "Here’s how we’ll proceed:\n",
        "1. I will provide you with a question and related text excerpt.\n",
        "2. Your task is to answer the question using only the provided partial texts.\n",
        "3. If the answer isn’t explicitly found within the given context,\n",
        "respond with 'I don't know'.\n",
        "4. After each response, please provide a detailed explanation.\n",
        "Break down your answer step by step and relate it directly to the provided context.\n",
        "5. Sometimes, I will ask questions about the chat session, such as summarize\n",
        "the chat or list the question etc. For this kind of questions do not try\n",
        "to use the provided partial texts.\n",
        "6. Generate the answer in the same language of the given question.\n",
        "\n",
        "If you're ready, I'll provide you with the question and the context.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_Kuf1IyHRUcP"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_LLM = build_chatBot(system_prompt)"
      ],
      "metadata": {
        "id": "1YgsKHxKSIlB"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Test the LLM connection"
      ],
      "metadata": {
        "id": "_ZOLUYaISRal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"What is FC?\"\n",
        "context= \"\"\"FC lets developers create a description\n",
        "of a F in their code, then pass that description to a language\n",
        "model in a request.\n",
        "\n",
        "The response from the model includes the name of\n",
        "a F that matches the description and the arguments to call it with.\n",
        "FC lets you use F as tools in generative AI applications,\n",
        "and you can define more than one F within a single request.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HmkPIU1jSW0s"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=generate_LLM_answer(prompt, context,RAG_LLM)\n",
        "to_markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "L3vtwTQlSNOW",
        "outputId": "47298904-8756-442a-8a69-68d6c10a5825"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> FC lets developers create a description of a F in their code, then pass that description to a language model in a request.\n> \n> **Explanation:**\n> 1.  **Identify the key term:** The question asks \"What is FC?\".\n> 2.  **Locate the term in the text:** The text begins with \"FC lets developers create a description...\".\n> 3.  **Extract the definition:** The sentence directly describes what FC is and its primary function. It states that \"FC lets developers create a description of a F in their code, then pass that description to a language model in a request.\""
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_LLM.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "678AEdaJS4We",
        "outputId": "ce351a09-8542-4fa5-c7a0-3218db713430"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"What is FC?FC lets developers create a description\\nof a F in their code, then pass that description to a language\\nmodel in a request.\\n\\nThe response from the model includes the name of\\na F that matches the description and the arguments to call it with.\\nFC lets you use F as tools in generative AI applications,\\nand you can define more than one F within a single request.\\n\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"FC lets developers create a description of a F in their code, then pass that description to a language model in a request.\\n\\n**Explanation:**\\n1.  **Identify the key term:** The question asks \\\"What is FC?\\\".\\n2.  **Locate the term in the text:** The text begins with \\\"FC lets developers create a description...\\\".\\n3.  **Extract the definition:** The sentence directly describes what FC is and its primary function. It states that \\\"FC lets developers create a description of a F in their code, then pass that description to a language model in a request.\\\"\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_LLM.history.clear()\n",
        "RAG_LLM.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeAwg653TGd1",
        "outputId": "a4b20566-a353-4118-923e-35e952b9726b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE THE RAG PIPELINE FOR THE EXISTING KNOWLEDGE BASE"
      ],
      "metadata": {
        "id": "jFM__cIVTO47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 A simple RAG Pipeline\n",
        "\n",
        "* preparea summary for the Knowledge Base\n",
        "* get the query from the user\n",
        "* query the Knowledge Base\n",
        "* get the related chunks from the Knowledge Base\n",
        "* combine the query + context from Knowledge Base\n",
        "* submit the prompt (query + context) to the LLM\n",
        "* get the response from the LLM"
      ],
      "metadata": {
        "id": "TKbz2uZPYJax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify collection properties\n",
        "print(f\"Collection name: {chroma_collection.name}\")  # Access the name attribute directly\n",
        "print(f\"Number of documents in collection: {chroma_collection.count()}\")\n",
        "\n",
        "# List all collections in the client\n",
        "print(\"All collections in ChromaDB client:\")\n",
        "for collection in chroma_client.list_collections():\n",
        "    print(collection.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owr3yfjnba2W",
        "outputId": "126a8327-6797-4199-c6df-5b60780d520e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection name: Papers\n",
            "Number of documents in collection: 199\n",
            "All collections in ChromaDB client:\n",
            "Papers\n"
          ]
        }
      ]
    },
    {
      "source": [
        "def summarize_collection(chroma_collection):\n",
        "  summary = [] # Initialize summary as a list\n",
        "  print(\"Summarizing the collection...\")\n",
        "  # Verify collection properties\n",
        "  print(f\"\\t Collection name: {chroma_collection.name}\")  # Access the name attribute directly\n",
        "  print(f\"\\t Number of document chunks in collection: {chroma_collection.count()}\")\n",
        "  summary.append(f\"Collection name: {chroma_collection.name}\") # Append to the list\n",
        "  summary.append(f\"Number of document chunks in collection: {chroma_collection.count()}\")\n",
        "  # Print distinct metadata \"document\" for each chunk in the collection\n",
        "  print(\"\\t Distinct 'document' metadata in the collection:\")\n",
        "  distinct_documents = set()  # Use a set to store unique document names\n",
        "\n",
        "  # Iterate over chunks in the collection\n",
        "  for chunk_id in range(chroma_collection.count()):\n",
        "      metadata = chroma_collection.get([str(chunk_id)])['metadatas'][0]  # Get metadata for the chunk\n",
        "      document_name = metadata.get(\"document\", \"Unknown\")  # Get document metadata; default to \"Unknown\" if not present\n",
        "      distinct_documents.add(document_name)  # Add document name to set for uniqueness\n",
        "\n",
        "  # Print all distinct document names\n",
        "  summary.append(\"Documents:\")\n",
        "  for document_name in distinct_documents:\n",
        "      print(\"\\t \",document_name)\n",
        "      summary.append(document_name) # Append to the list\n",
        "\n",
        "  print(\"Collection summarization completed.\")\n",
        "\n",
        "  # Join the list elements into a single string\n",
        "  summary_string = \"\\n \".join(summary)\n",
        "  return summary_string"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RYSGrIVMsUNf"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=summarize_collection(chroma_collection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-7CShpCcrX2",
        "outputId": "7bcf8b0d-aefe-420a-fd6a-60cbe7645bc8"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing the collection...\n",
            "\t Collection name: Papers\n",
            "\t Number of document chunks in collection: 199\n",
            "\t Distinct 'document' metadata in the collection:\n",
            "\t  peerj-cs-93 (2).pdf\n",
            "\t  1912.10819v1 (2).pdf\n",
            "Collection summarization completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIYVjyfhshd2",
        "outputId": "905ade8b-b184-4f7b-b1e0-ced85210af95"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection name: Papers\n",
            " Number of document chunks in collection: 199\n",
            " Documents:\n",
            " peerj-cs-93 (2).pdf\n",
            " 1912.10819v1 (2).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generateAnswer(RAG_LLM, chroma_collection,query,n_results=5, only_response=True):\n",
        "    retrieved_documents= retrieveDocs(chroma_collection, query, 10, return_only_docs=True)\n",
        "    prompt = \"QUESTION: \"+ query\n",
        "    context = \"\\n EXCERPTS: \"+ \"\\n\".join(retrieved_documents)\n",
        "    if not only_response:\n",
        "      print(\"------- retreived documents -------\\n\")\n",
        "      for i, doc in enumerate(retrieved_documents):\n",
        "        print(f\"Document {i+1}:\")\n",
        "        print(f\"\\tDocument Text: {doc}\")\n",
        "      print(\"------- RAG answer -------\\n\")\n",
        "    output = generate_LLM_answer(prompt, context, RAG_LLM)\n",
        "\n",
        "    display(to_markdown(output))\n",
        "    print('\\n')\n",
        "    return output"
      ],
      "metadata": {
        "id": "sHBw828MYHbU"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Test the RAG pipeline"
      ],
      "metadata": {
        "id": "i87q584BY_l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries =[\"Who are the authors suggested a new attention mechanism?\",\n",
        "          \"Who are the authors suggested a new controllable text generation mechanism?\",\n",
        "          \"Who is Murat Karakaya?\",\n",
        "          \"Why do we need to control how the text is produced? \",\n",
        "          \"How can we use the self attention mechanism to control the text generation?\",\n",
        "          \"Summarize the paper named Controllable Text Generation\",\n",
        "          \"How many blocks are suggested in the transformer?\",\n",
        "          \"What about decoder?\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "IQuoIhc_URCD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply=generateAnswer(RAG_LLM, chroma_collection, queries[2],10, only_response=False)"
      ],
      "metadata": {
        "id": "eNGtY7_qRHRl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "1bd26f5b-412a-4c6c-a607-0a24f0179e35"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- retreived documents -------\n",
            "\n",
            "Document 1:\n",
            "\tDocument Text: 1An amicus curiae ( friend of the court ) is a person or organisation that offers testimony before the Court in the context of a particular case without being a formal party to the proceedings. In this paper, our particular focus is on the automatic analysis of cases of the European Court of Human Rights ( ECtHR or Court ). The ECtHR is an international court that rules on individual or, much more rarely, State applications alleging violations by some State Party of the civil and political rights set out in the European Convention on Human Rights ( ECHR or Convention ). Our task is to pred\n",
            "Document 2:\n",
            "\tDocument Text: . nips. cc / paper / 5872 - efficient - and - robust - automated - machine - learning. pdf Guimera, R., Sales [UNK] Pardo, M. : Justice blocks and predictability of us supreme court votes. PloS one 6 ( 11 ), e27188 ( 2011 ) Katz, D. M., Bommarito II, M. J., Blackman, J. : A general approach for predicting the behavior of the supreme court of the united states. PloS one 12 ( 4 ), e0174698 ( 2017\n",
            "Document 3:\n",
            "\tDocument Text: task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i. e., N - grams, and topics. Our models can predict the court [UNK] s decisions with a strong accuracy ( 79 % on average ). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision -\n",
            "Document 4:\n",
            "\tDocument Text: support vector machines : methods, theory and algorithms. Kluwer Academic Publishers.\n",
            "Document 5:\n",
            "\tDocument Text: 93 7 / 19\n",
            "Document 6:\n",
            "\tDocument Text: ) Kaufman, A., Kraft, P., Sen, M. : Machine learning, text data, and supreme court forecasting. Project Report, Harvard University ( 2017 ) Kuhn, M., Johnson, K. : Applied Predictive Modeling. Springer, Springer New York Heidelberg Dordrecht London ( 2013 ) Le, Q., Mikolov, T. : Distributed representations of sentences and documents. In : International conferenceon machine learning. pp. 1188 [UNK] 1196 ( 2014 )\n",
            "Document 7:\n",
            "\tDocument Text: Table 5 The most predictive topics for Article 8 decisions. Most predictive topics for Article 8, represented by the 20 most frequent words, listed in order of their SVM weight. Topic labels are manually added. Positive weights ( w ) denote more predictive topics for violation and negative weights for no violation. Topic Label Words w Top - 5 Violation 30 Death and military action son, body, result, russian, department, prosecutor office, death, group, relative, head, described, military, criminal investigation, burial, district prosecutor, men, deceased, town,\n",
            "Document 8:\n",
            "\tDocument Text: shows the mean accuracy across the three articles. In general, both N - gram and topic features achieve good predictive performance. Our main observation is that both language use and topicality are important factors that appear to stand as\n",
            "Document 9:\n",
            "\tDocument Text: B. 2010. Legal formalism and legal realism : what is the issue? Legal Theory 16 ( 2 ) : 111 [UNK] 133 DOI 10. 1017 / S1352325210000121. Llewellyn KN. 1996. The common law tradition : deciding appeals. William S. Hein & Co., Inc.. Miles TJ, Sunstein CR. 2008. The new legal realism. The University of Chicago Law Review 75 ( 2 ) : 831 [UNK] 851. Nagel SS. 1963. Applying correlation analysis to case prediction. Texas Law Review 42 : 1006. Pildes\n",
            "Document 10:\n",
            "\tDocument Text: ##a Kompaniya v. Russia of 7 June 2007. Consequently, the topics identify independently well - established trends in the case law without recourse to expert legal / doctrinal analysis. The above observations require to be understood in a more mitigated way with respect to a ( small ) number of topics. For instance, most representative cases for topic 8 in Table 3 were not particularly informative. This is because these were cases involving a person [UNK] s death, in which claims of violations of Article 3 ( inhuman and degrading treatment ) were only subsidiary : this means that the claims were mainly about Article 2,\n",
            "------- RAG answer -------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> I don't know.\n> \n> **Explanation:**\n> The provided text excerpts discuss topics related to the European Court of Human Rights, machine learning applications in legal prediction, legal theories like formalism and realism, and various research papers and authors. I have carefully reviewed all the provided text, including the body of the text and the bibliographic references. The name \"Murat Karakaya\" is not mentioned or identified in any part of the given context."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 A simple loop for the User Interaction"
      ],
      "metadata": {
        "id": "MMMesAl_TjHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_collection(chroma_collection)\n",
        "RAG_LLM.history.clear()\n",
        "while True:\n",
        "  question = input(\"Please enter your question, or type 'bye' to exit: \")\n",
        "  if question == \"bye\":\n",
        "    print(\"Thank you for using the service. Goodbye!\")\n",
        "    break\n",
        "  else:\n",
        "    generateAnswer(RAG_LLM, chroma_collection, question)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y6fOinIoTt5e",
        "outputId": "8c9eb6b3-64b0-4448-a8a9-1ea6901b62b6"
      },
      "execution_count": 109,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarizing the collection...\n",
            "\t Collection name: Papers\n",
            "\t Number of document chunks in collection: 199\n",
            "\t Distinct 'document' metadata in the collection:\n",
            "\t  peerj-cs-93 (2).pdf\n",
            "\t  1912.10819v1 (2).pdf\n",
            "Collection summarization completed.\n",
            "Please enter your question, or type 'bye' to exit: what is echr?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The ECHR is an international treaty established for the protection of civil and political liberties in European democracies that are committed to the rule of law. It was initially drafted in 1950 by the ten states that had created the Council of Europe the previous year and entered into force in 1953. Membership in the Council of Europe requires becoming a party to the Convention.\n> \n> **Explanation:**\n> 1.  **\"The ECHR is an international treaty...\"**: This directly states what ECHR is, as found in the first provided excerpt.\n> 2.  **\"...for the protection of civil and political liberties in European democracies committed to the rule of law.\"**: This explains the purpose and scope of the ECHR, also directly from the first excerpt.\n> 3.  **\"It was initially drafted in 1950 by the ten states which had created the Council of Europe in the previous year and entered into force in 1953.\"**: This provides information about its origin and when it became active, directly sourced from the first excerpt.\n> 4.  **\"Membership in the Council entails becoming party to the Convention...\"**: This details the relationship between the ECHR and the Council of Europe, again from the first excerpt."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Please enter your question, or type 'bye' to exit: echr nedir?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ECHR (Avrupa İnsan Hakları Sözleşmesi), hukukun üstünlüğüne bağlı Avrupa demokrasilerinde sivil ve siyasi özgürlüklerin korunmasına yönelik uluslararası bir antlaşmadır. Antlaşma, başlangıçta 1950 yılında, bir önceki yıl Avrupa Konseyi'ni kuran on devlet tarafından hazırlanmıştır ve 1953 yılında yürürlüğe girmiştir. Avrupa Konseyi'ne üyelik, bu Sözleşme'ye taraf olmayı gerektirir. ECHR ayrıca, Avrupa İnsan Hakları Sözleşmesi'nin potansiyel ihlallerini inceleyen uluslararası bir mahkeme olan Avrupa İnsan Hakları Mahkemesi'ni de ifade edebilir.\n> \n> **Açıklama:**\n> 1.  **\"The ECHR is an international treaty for the protection of civil and political liberties in European democracies committed to the rule of law.\"** (ECHR, hukukun üstünlüğüne bağlı Avrupa demokrasilerinde sivil ve siyasi özgürlüklerin korunmasına yönelik uluslararası bir antlaşmadır.) ifadesi, ECHR'nin ne olduğunu ve amacını ilk paragraftan alıntılar.\n> 2.  **\"The treaty was initially drafted in 1950 by the ten states which had created the Council of Europe in the previous year. The Convention itself entered into force in 1953.\"** (Antlaşma, başlangıçta 1950 yılında, bir önceki yıl Avrupa Konseyi'ni kuran on devlet tarafından hazırlanmıştır ve 1953 yılında yürürlüğe girmiştir.) bilgisi, antlaşmanın kökeni ve yürürlüğe giriş tarihini yine ilk paragraftan sağlar.\n> 3.  **\"Membership in the Council entails becoming party to the Convention...\"** (Avrupa Konseyi'ne üyelik, bu Sözleşme'ye taraf olmayı gerektirir.) ifadesi, Konsey üyeliği ile Sözleşme arasındaki ilişkiyi ilk paragraftan belirtir.\n> 4.  **\"The European Court of Human Rights ( ECHR ) is an international court that examines potential breaches of the European Convention on Human Rights.\"** (Avrupa İnsan Hakları Mahkemesi (AİHM - ECHR), Avrupa İnsan Hakları Sözleşmesi'nin potansiyel ihlallerini inceleyen uluslararası bir mahkemedir.) bilgisi ise, ECHR'nin aynı zamanda bir mahkeme olarak da anıldığı ve görevi hakkında ek bir tanımı metinlerden sağlamıştır."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Please enter your question, or type 'bye' to exit: bye\n",
            "Thank you for using the service. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 A Gradio Interface"
      ],
      "metadata": {
        "id": "9TCXZkVWTUF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gradio\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "LDfjbm8JiVE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7030ed-176a-4f71-a571-f970d23a7b83"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.128.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (26.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.22)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.15.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.4.2)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_LLM.history.clear()\n",
        "\n",
        "# Replace with your actual function (assuming it generates an answer)\n",
        "def generateAnswerInterFace(question):\n",
        "    return generateAnswer(RAG_LLM, chroma_collection, question)\n",
        "\n",
        "# Function to generate the info text\n",
        "def get_info_text():\n",
        "    return \"INFO: \" + summarize_collection(chroma_collection)\n",
        "    # Assuming summarize_collection returns a string\n",
        "\n",
        "# Use gr.Blocks instead of gr.Interface\n",
        "with gr.Blocks() as demo:\n",
        "    # Define interface components\n",
        "    query_txt = gr.Textbox(label=\"Enter your question here:\", placeholder=\"Type your question\")\n",
        "    answer_txt = gr.Textbox(label=\"Answer:\", placeholder=\"Answer will be displayed here\")\n",
        "\n",
        "    # Create a button to trigger the prediction\n",
        "    btn = gr.Button(\"Generate Answer\")\n",
        "\n",
        "    # Define the prediction function (order changed for button placement)\n",
        "    def predict(question):\n",
        "        answer = generateAnswerInterFace(question)\n",
        "        return answer\n",
        "\n",
        "    info_txt = gr.Textbox(get_info_text(), label=\"Info\")  # Add info textbox after button\n",
        "\n",
        "    # Connect button click to prediction function\n",
        "    btn.click(predict, inputs=query_txt, outputs=answer_txt)\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "1qt3Tgf8vwdL",
        "outputId": "d47a34f7-a613-46b1-9f64-2bcc9635b6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing the collection...\n",
            "\t Collection name: Papers\n",
            "\t Number of document chunks in collection: 199\n",
            "\t Distinct 'document' metadata in the collection:\n",
            "\t  peerj-cs-93 (2).pdf\n",
            "\t  1912.10819v1 (2).pdf\n",
            "Collection summarization completed.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e9dbff865d28cc78e7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e9dbff865d28cc78e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_LLM.history"
      ],
      "metadata": {
        "id": "oB0go7_LzFTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUMMARY"
      ],
      "metadata": {
        "id": "1i-A6j2_8mJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHY WE NEED A PERSISTENT CHROMADB?\n",
        "In the context of a Retrieval-Augmented Generation (RAG) approach, saving and loading a persistent ChromaDB is particularly important for several reasons:\n",
        "\n",
        "Enhanced Data Durability:\n",
        "\n",
        "Importance: Ensures the retrieval database used for augmenting generative models is not lost between sessions or system restarts.\n",
        "RAG Relevance: Maintains a consistent and reliable knowledge base that the generative model can reference, leading to more accurate and relevant responses.\n",
        "Operational Continuity:\n",
        "\n",
        "Importance: Allows seamless continuation of operations without needing to re-index or re-import data, saving time and computational resources.\n",
        "RAG Relevance: Ensures that the generative model has continuous access to the same set of documents, which is essential for generating consistent and coherent responses over time.\n",
        "Facilitating Collaboration:\n",
        "\n",
        "Importance: Enables multiple users or systems to share and access the same dataset.\n",
        "RAG Relevance: Supports collaborative development and usage of the RAG system, allowing different teams to work on improving the retrieval and generation processes simultaneously.\n",
        "Scalability:\n",
        "\n",
        "Importance: Provides a stable and persistent backend, enabling efficient handling of large datasets.\n",
        "RAG Relevance: Essential for scaling the RAG system to handle more extensive and diverse knowledge bases, ensuring that the system can manage increased loads and deliver prompt, relevant information.\n",
        "In a RAG system, the retriever (like ChromaDB) provides the generative model with relevant context from a knowledge base to generate informed and accurate responses. Persistent storage ensures that this knowledge base is durable, continuously available, and scalable, which is critical for the reliability, consistency, and performance of the RAG system."
      ],
      "metadata": {
        "id": "qtqN4k77Plu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "Y9Xi2_79A254"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "38itsCX9A2t8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "mCjijo4pA2f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateAnswer(RAG_LLM, chroma_collection,query,n_results=5):\n",
        "    retrieved_documents= retrieveDocs(chroma_collection, query, 10, return_only_docs=True)\n",
        "\n",
        "    print(\"------- retreived documents -------\\n\")\n",
        "    for i, doc in enumerate(retrieved_documents):\n",
        "      print(f\"Document {i+1}:\")\n",
        "      print(f\"\\tDocument Text: {doc}\")\n",
        "    prompt = \"QUESTION: \"+ query\n",
        "    context = \"\\n EXCERPTS: \"+ \"\\n\".join(retrieved_documents)\n",
        "    print(\"------- RAG answer -------\\n\")\n",
        "    output = generate_LLM_answer(prompt, context, RAG_LLM)\n",
        "\n",
        "    display(to_markdown(output))\n",
        "    print('\\n')\n",
        "    return output"
      ],
      "metadata": {
        "id": "EOpIzjPEQpvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(reply.text)"
      ],
      "metadata": {
        "id": "_BlrLcyATatn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n"
      ],
      "metadata": {
        "id": "Vi3g2vgMRG81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.count_tokens(chat.history)"
      ],
      "metadata": {
        "id": "8Lu-LtxiW9Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(prompt)\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "id": "WKX4tzgsNijg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0QKtnH7NNFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "'''\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai_client = OpenAI()\n",
        "'''\n",
        "openai_client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "iz7ZbNXaMyY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query, retrieved_documents, model=\"gpt-3.5-turbo-1106\"):\n",
        "    information = \"\\n\\n\".join(retrieved_documents)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"As an attentive and supportive academic assistant, \"\n",
        "            \"your task is to provide assistance based solely on the provided\"\n",
        "            \" excerpts. Answer the following questions, ensuring your responses\"\n",
        "            \" are derived exclusively from the provided partial texts. \"\n",
        "            \"If the answer cannot be found within the provided excerpts, \"\n",
        "            \"kindly respond with 'I don't know'.\"\n",
        "            \"After answering each question, please provide a detailed \"\n",
        "            \"explanation, breaking down the answer step by step and relating \"\n",
        "            \"it to the provided excerpts.\"\n",
        "            \"Return your response as a Json object with two key fields: \"\n",
        "            \" 'Answer', which should contain the value of the answer, and \"\n",
        "            \" 'Reason', which should provide an explanation of why this answer \"\n",
        "            \"was generated.\"\n",
        "\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Excerpts: {information}\"}\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content"
      ],
      "metadata": {
        "id": "GGb0tQvtM51N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5K440G_M8cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateAnswer(query,n_results=5):\n",
        "    retrieved_documents=retrieveDocs(query,n_results)\n",
        "\n",
        "    print(\"------- retreived documents -------\\n\")\n",
        "    for i, doc in enumerate(retrieved_documents):\n",
        "      print(f\"Document {i+1}:\")\n",
        "      print(f\"\\tDocument Text: {doc}\")\n",
        "    print(\"------- RAG answer -------\\n\")\n",
        "    output = rag(query=query, retrieved_documents=retrieved_documents)\n",
        "    print(output)\n",
        "    print('\\n')\n",
        "    return output"
      ],
      "metadata": {
        "id": "CJd-S0zcRaFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xb9uHSMpTR_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2d-jQLvLufI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply=generateAnswer(queries[5],10)\n"
      ],
      "metadata": {
        "id": "sTcewpPcWQLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert the 'reply' to a dict\n",
        "\n",
        "import ast\n",
        "reply_dict = ast.literal_eval(reply)\n",
        "print(f\"Answer: {reply_dict['Answer']}\")\n",
        "print(f\"Because; {reply_dict['Reason']}\")"
      ],
      "metadata": {
        "id": "MSIFzsUtRBOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries:\n",
        "  generateAnswer(query)"
      ],
      "metadata": {
        "id": "trkwnbPTWPAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install umap-learn"
      ],
      "metadata": {
        "id": "ToFXpONGbhc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def project_embeddings(embeddings, umap_transform):\n",
        "    umap_embeddings = np.empty((len(embeddings),2))\n",
        "    for i, embedding in enumerate(tqdm(embeddings)):\n",
        "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
        "    return umap_embeddings"
      ],
      "metadata": {
        "id": "6jIGoTEeqZnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap.umap_ as umap\n",
        "\n",
        "embeddings = chroma_collection.get(include=['embeddings'])['embeddings']\n",
        "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\n",
        "projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)"
      ],
      "metadata": {
        "id": "eq8Oxx_SXJK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10)\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title('Projected Embeddings')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "PAmWc2acstG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = queries[3]\n",
        "\n",
        "results = chroma_collection.query(query_texts=query, n_results=10, include=['documents', 'embeddings'])\n",
        "\n",
        "retrieved_documents = results['documents'][0]\n",
        "\n",
        "for document in results['documents'][0]:\n",
        "    print(document)\n",
        "    print('')\n"
      ],
      "metadata": {
        "id": "mw19GvOZpyO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = embedding_function([query])[0]\n",
        "retrieved_embeddings = results['embeddings'][0]\n",
        "\n",
        "projected_query_embedding = project_embeddings([query_embedding], umap_transform)\n",
        "projected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)\n"
      ],
      "metadata": {
        "id": "J7ClIPkh-unn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the projected query and retrieved documents in the embedding space\n",
        "plt.figure()\n",
        "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
        "plt.scatter(projected_query_embedding[:, 0], projected_query_embedding[:, 1], s=150, marker='X', color='r')\n",
        "plt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
        "\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title(f'{query}')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "_FGgCd8t-23T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_query_generated(query, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Sen TÜBİTAK proje başvurularını inceleyen yapay zeka konusunda uzman bir akasemisyensin.\"\n",
        "            \"Aşağıda verilen soruya, aşağıdaki proje tanımına uygun olabilecek bir cevap üret: \\n\"\n",
        "            \"Projenin genel amacı, bankacılık sektöründeki risk yönetimi operasyonlarını geliştirmek ve finansal kurumların karşılaştığı zorlukları ele almak \"\n",
        "            \"için yapay zeka (AI) tabanlı bir platform geliştirmektir. Proje, bankalara vadeli mevduatın erken bozulması, kredilerin erken ödenmesi ve çeşitli \"\n",
        "            \"mevduat türlerinin belirlenmesi gibi davranışsal riskleri daha etkili bir şekilde yönetme kapasitesi sunmayı hedeflemektedir. Bu riskler, finansal \"\n",
        "            \"kurumların bilanço dengesini etkileyebilir ve operasyonel verimliliği azaltabilir. \"\n",
        "            \"Projenin çözmeyi amaçladığı temel problem, bankaların karlılık ve risk analizlerini gerçekleştirirken karşılaştığı karmaşık durumları doğru ve \"\n",
        "            \"etkili bir şekilde yönetme ihtiyacıdır. Özellikle vadeli mevduatların erken kapanması ve kredilerin erken ödenmesi gibi durumlar, bankaların \"\n",
        "            \"gelecekteki nakit akışlarını ve risk profillerini belirleme sürecini karmaşıklaştırabilir\"\n",
        "\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content"
      ],
      "metadata": {
        "id": "MAgqvNgvAE_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_query = queries[0]\n",
        "hypothetical_answer = augment_query_generated(original_query)\n",
        "\n",
        "joint_query = f\"{original_query} {hypothetical_answer}\"\n",
        "print(joint_query)"
      ],
      "metadata": {
        "id": "pVgmXEmDA4Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_retrieved_documents(results, extension=4):\n",
        "  original_ids= results['ids'][0]\n",
        "  print(\"original_ids: \",original_ids)\n",
        "\n",
        "  extended_ids = set()\n",
        "\n",
        "\n",
        "  for id in original_ids:\n",
        "    extended_ids.add(int(id))\n",
        "    for i in range(1, extension):\n",
        "      extended_ids.add(int(id)+i)\n",
        "\n",
        "\n",
        "  extended_ids = sorted([int(x) for x in extended_ids])\n",
        "  extended_ids = [str(x) for x in extended_ids if int(x) < chroma_collection.count()]\n",
        "  print(\"extended_ids: \",extended_ids)\n",
        "  return chroma_collection.get(extended_ids)['documents']"
      ],
      "metadata": {
        "id": "LG7hu7wDdrMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieveDocs_augmented_query(query, n_results=5, extension=4):\n",
        "    hypothetical_answer = augment_query_generated(query)\n",
        "    print(\"------ hypothetical_answer ---------\\n\")\n",
        "    print(hypothetical_answer,\"\\n\")\n",
        "    print(\"------------------------------------\\n\")\n",
        "    joint_query = f\"{query} {hypothetical_answer}\"\n",
        "    results = chroma_collection.query(query_texts=joint_query, n_results=n_results, include=['documents', 'embeddings'])\n",
        "    retrieved_documents = extend_retrieved_documents(results, extension)\n",
        "    #retrieved_documents = results['documents'][0]\n",
        "\n",
        "    return retrieved_documents\n",
        "\n"
      ],
      "metadata": {
        "id": "5_tnGca2D9CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_documents=retrieveDocs_augmented_query(query, 5)\n",
        "\n",
        "for doc in retrieved_documents:\n",
        "    print(doc)\n",
        "    print('')"
      ],
      "metadata": {
        "id": "PXqwdlB3ElTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = chroma_collection.query(query_texts=joint_query, n_results=10, include=['documents', 'embeddings'])\n",
        "retrieved_documents = results['documents'][0]\n",
        "\n",
        "for doc in retrieved_documents:\n",
        "    print(doc)\n",
        "    print('')"
      ],
      "metadata": {
        "id": "ScyronsnBIsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_embeddings = results['embeddings'][0]\n",
        "original_query_embedding = embedding_function([original_query])\n",
        "augmented_query_embedding = embedding_function([joint_query])\n",
        "\n",
        "projected_original_query_embedding = project_embeddings(original_query_embedding, umap_transform)\n",
        "projected_augmented_query_embedding = project_embeddings(augmented_query_embedding, umap_transform)\n",
        "projected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)"
      ],
      "metadata": {
        "id": "ncz_Vs8sBez8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the projected query and retrieved documents in the embedding space\n",
        "plt.figure()\n",
        "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
        "plt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
        "plt.scatter(projected_original_query_embedding[:, 0], projected_original_query_embedding[:, 1], s=150, marker='X', color='r')\n",
        "plt.scatter(projected_augmented_query_embedding[:, 0], projected_augmented_query_embedding[:, 1], s=150, marker='X', color='orange')\n",
        "\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title(f'{original_query}')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "s7IiB6pTBllH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spIkCz0SdRYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateAnswer_augmented_query(query,n_results=5, extention=4):\n",
        "    print(\"------- query -------\\n\")\n",
        "    print(query,\"\\n\")\n",
        "    retrieved_documents=retrieveDocs_augmented_query(query,n_results,extention)\n",
        "    print(\"------- retreived documents -------\\n\")\n",
        "    for document in retrieved_documents:\n",
        "        print(document)\n",
        "        print('\\n')\n",
        "\n",
        "    print(\"------- RAG answer -------\\n\")\n",
        "    output = rag(query=query, retrieved_documents=retrieved_documents)\n",
        "    print(output)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "O_jYC08eFZJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries"
      ],
      "metadata": {
        "id": "ZoDGrmz9mv_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generateAnswer_augmented_query(queries[0],10,5)"
      ],
      "metadata": {
        "id": "fU61GVoIFpud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title= \"\"\"Ar-Ge Sürecinde Kullanılacak Yöntemler Tanımlanan proje hedeflerine ulaşmak için uygulanacak analitik\n",
        "        deneysel çözüm yöntemlerini belirtiniz. (NOT: Bu bölümde sunulan proje özelinde\n",
        "        hangi teknik / bilimsel yaklaşımların ve bunlara ait aşamaların takip edileceği açıklanmalı, iş paketleri isimleri ya da her projede olabilecek standart\n",
        "        rutin çalışma yöntemleri tekrarlanmamalıdır.\"\"\"\n",
        "results = chroma_collection.query(query_texts=title, n_results=5, include=['documents', 'embeddings'])\n",
        "retrieved_documents = results['documents'][0]\n",
        "print(retrieved_documents)"
      ],
      "metadata": {
        "id": "6bJGG3mEUFKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7Yq67GcUyYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tNjnJTDvVXpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title= \"\"\"Ar-Ge Sürecinde Kullanılacak Yöntemler Tanımlanan proje hedeflerine ulaşmak için uygulanacak analitik\n",
        "        deneysel çözüm yöntemlerini belirtiniz. (NOT: Bu bölümde sunulan proje özelinde\n",
        "        hangi teknik / bilimsel yaklaşımların ve bunlara ait aşamaların takip edileceği açıklanmalı, iş paketleri isimleri ya da her projede olabilecek standart\n",
        "        rutin çalışma yöntemleri tekrarlanmamalıdır.\"\"\"\n",
        "results = chroma_collection.query(query_texts=title, n_results=5, include=['documents', 'embeddings'])\n",
        "\n",
        "retrieved_documents = extend_retrieved_documents(results)\n",
        "print(retrieved_documents)\n"
      ],
      "metadata": {
        "id": "_pzqUkWQWqkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mg5KFoacc67g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_collection.get(results['ids'][0])"
      ],
      "metadata": {
        "id": "WrRDvmYSVlLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}